% \documentclass[11pt,conference]{IEEEtran}
\documentclass[sigconf]{acmart}

\usepackage{bm}
\usepackage{listings}
\usepackage{tikz}
\usepackage[]{algorithm2e}
\usepackage{subcaption}
\usepackage{xcolor}

\usetikzlibrary{matrix, fit}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{cd}

\input{models/alloy.tex}

\begin{document}
\pagestyle{plain}
\title{SpMV Sections}
\maketitle

\section{Sparse Matrix-Vector Multiplication}

Sparse linear algebra libraries and the methods they employ are vitally important in the domain of scientific computing (TODO: why?).  Of particular interest is the sparse matrix-vector product, which solves $y = \bm{A}x$ where $\bm{A}$ is a sparse matrix and the vectors $y$ and $x$ are dense.  Because SpMV usage is often highly repetitive within, e.g., iterative solvers, performance improvements via novel algorithms and the exploitation of modern hardware are areas of ongoing research (TODO: citations).

...why is SpMV difficult? Things like array indirection, varying sparsity patterns, it's memory bound...

As modern hardware introduces higher levels of parallelism and SpMV algorithms become more complex, the likelihood of introducing subtle bugs becomes decidedly higher.  In this section we present a method for reasoning about the structural complexities and verifying the correctness of SpMV algorithms.  We first model an abstract matrix-vector multiplication and subsequently demonstrate that a CSR SpMV algorithm is a valid functional refinement.

\subsection{Abstract Matrix-Vector Multiplication}

In order to demonstrate that an SpMV algorithm is correct, we must first create a model of matrix-vector multiplication to act as the arbiter of correctness.  The result of the matrix-vector multiplication $y = \bm{A}x$ is a densely populated vector, $y$, in which each value is the dot product of a single row of the matrix $A$ with the vector $x$.  A dot product is simply a sum of products in which each product is of two values located at the same index within two vectors, as shown in ~\figurename~\ref{fig:mvm}.

\begin{figure}
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \input{images/mvm.tex}
  \caption{}
  \label{fig:mvm}
\end{subfigure}
{\color{lightgray}\rule{0.4\textwidth}{0.1pt}}
\par\bigskip
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \input{images/dotprod.tex}
  \caption{}
  \label{fig:dp}
\end{subfigure}
{\color{lightgray}\rule{0.4\textwidth}{0.1pt}}
\par\bigskip
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \input{images/sumprod.tex}
  \caption{}
  \label{fig:dpt}
\end{subfigure}
\caption{(\subref{fig:mvm}) a matrix-vector multiplication, (\subref{fig:dp}) the components of the first dot product in (\subref{fig:mvm}), and (\subref{fig:dpt}) the relational form of the same dot product.}
\end{figure}

Thus far, our matrix and sparse matrix models are capable of representing the structure of the matrix and vectors, as well as the values contained in $\bm{A}$ and $x$.  However, because we wish to model the \emph{structural} behavior of the algorithm, the resulting vector, $y$, cannot simply contain abstract values.  Some mechanism that allows us to determine the origin of values in the dot product is required so that we can reason about whether or not the dot product is the correct composition of values.  This is achieved using the \texttt{DotProd} signature, introduced in Section~\ref{sec:dotprod}.  Additionally, we require some mechanism for equating dot products, allowing us to determine if the solutions generated by differing algorithms are equivalent.  This is achieved using the \texttt{valEqv} predicate introduced in Section~\ref{sec:valeqv}.

\subsubsection{Dot Product Model}
\label{sec:dotprod}

Hi.
\begin{figure}
\input{models/sumprod.tex}
\end{figure}

\subsubsection{Dot Product Equivalence}
\label{sec:valeqv}

Hi.

\bibliography{sparse}

\end{document}